<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/">
  <channel>
    <title>JAY</title>
    <link>https://jkeun.github.io</link>
    <atom:link href="https://jkeun.github.io/feed.xml" rel="self" type="application/rss+xml" />
    
      <item>
        <title>Bias-Variance Tradeoff</title>
        <description>&lt;center&gt;&lt;mark&gt;&quot;In statistics and machine learning, the &lt;b&gt;bias-variace tradeoff&lt;/b&gt; is the problem of simultaneously minimizing two sources of error that prevent supervised learning algorithms from generalizing beyond their training set.&quot;&lt;/mark&gt;&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;bias-and-variance-tradeoff&quot;&gt;Bias and Variance Tradeoff&lt;/h3&gt;
&lt;p&gt;통계학과 머신러닝에서 예측모형에 대해서 이야기할 때, 예측 에러는 크게 두가지 부분으로 나뉜다: “Bias(편향)”에 의한 에러와  “Variance(분산)”에 의한 에러. 그러나 불행하게도 &lt;a href=&quot;https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff#cite_note-6&quot;&gt;Bias-variance tradeoff&lt;/a&gt;로 인해 양 쪽 모두를 동시에 줄일 수 없으며 또한 이는 모형의 Overfitting(과적합) 또는 Underfitting(과소적합) 문제와도 연결된다. 따라서 뛰어난 모형은 편향과 분산의 총 합을 줄이기 위해 노력해야하며, 이 두 종류의 에러를 이해하는 것은 결국 Over/Under fitting의 실수를 피하고 예측모형의 올바른 진단을 이끌어내는데 도움이 될 것이다.&lt;/p&gt;

&lt;h3 id=&quot;conceptual--mathematical-definition&quot;&gt;Conceptual &amp;amp; Mathematical Definition&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Error due to Bias&lt;/strong&gt;: 편향에 의한 에러는 우리의 prediction function(&lt;script type=&quot;math/tex&quot;&gt;\hat {f}&lt;/script&gt;)과 찾고자 하는 ture function(&lt;script type=&quot;math/tex&quot;&gt;f&lt;/script&gt;)과의 차이로 인해 발생한다. 아래 수식을 보면 편향은 그 차이의 기댓값이(expected, average)임을 알 수 있는데, 그 이유는 다수의 샘플링된 데이터셋(&lt;script type=&quot;math/tex&quot;&gt;D_i&lt;/script&gt;)마다 다수의 prediction function(&lt;script type=&quot;math/tex&quot;&gt;\hat {f_i}&lt;/script&gt;)이 존재하기 때문이다.&lt;br /&gt;
즉 편향은 알고리즘을 학습하는데 있어서 가정 및 학습의 방향성을 의미한다고 볼 수 있으며, 만약 편향에 의한 에러가 크다면 잘못된 방향으로의 학습을 뜻하고, 우리의 prediction function이 주어진 데이터셋에서 features와 target간의 관계를 잘 파악하지 못한 것이라 할 수 있다. (&lt;strong&gt;Underfitting&lt;/strong&gt;)&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\operatorname {Bias} {\big [}{\hat {f}}(x){\big ]}=\operatorname {E} {\big [}{\hat {f}}(x)-f(x){\big ]}&lt;/script&gt;

&lt;p&gt;&lt;strong&gt;Error due to variance&lt;/strong&gt;: 분산에 의한 에러는 주어진 우리의 prediction function들의 다양한 정도, 분산의 정도에 의해 발생한다. 다시말해 실제값(actual point)에 대하여 우리의 prediction function들이 얼마나 다양한 범위로 예측을 하는지이다.&lt;br /&gt;
즉 분산은 알고리즘을 학습하는데 있어서 학습의 일관성을 의미한다고 볼 수 있으며, 만약 분산에 의한 에러가 크다면 각각의 알고리즘 학습이 일관성 없이 중구난방으로 이뤄졌음을 뜻하고, 우리의 prediction funtion이 학습용 데이터(training data)에 포함된 노이즈까지 학습한 것이라 볼 수 있다. (&lt;strong&gt;Overfitting&lt;/strong&gt;)&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
\operatorname {Var} {\big [}{\hat {f}}(x){\big ]}= &amp; \operatorname {E} {\big [} (\hat {f}(x) - \operatorname{E}[\hat{f}(x)])^2{\big ]} \\
&amp; \operatorname {E} [{\hat {f}}(x)^{2}]-\operatorname {E} [{\hat {f}}(x)]^{2} \\ 
\end{align} %]]&gt;&lt;/script&gt;

&lt;h3 id=&quot;graphical-definition&quot;&gt;Graphical Definition&lt;/h3&gt;
&lt;p&gt;편향과 분산에 대해서 직관적으로 파악할 수 있는 bulls-eye diagram을 보자. 중앙의 빨간색 목표물이 바로 우리의 모델이 예측해야하는 실제값(&lt;script type=&quot;math/tex&quot;&gt;y&lt;/script&gt;)이며, 목표물에서 멀어지면 멀어질 수록 예측력이 점점 더 나빠짐을 의미한다. 파란색 점들은 샘플링된 여러 데이터셋에 대한 prediction model들의 예측값(&lt;script type=&quot;math/tex&quot;&gt;y_i&lt;/script&gt;, when &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt; is # of models)들을 의미한다.&lt;br /&gt;
편향의 정도에 따라 파란색 점들이 빨간색 목표물로 부터 얼마나 떨어져 있는지를 확인할 수 있고, 분산의 정도에 따라 파란색 점들이 얼마나 흩어져 있는지를 확인할 수 있다.&lt;/p&gt;

&lt;center&gt;&lt;img src=&quot;/images/bias-variance-0.png&quot; width=&quot;400&quot; /&gt;&lt;/center&gt;

&lt;h3 id=&quot;bias-variance-decomposition&quot;&gt;Bias-Variance Decomposition&lt;/h3&gt;

&lt;p&gt;우리의 훈련용 데이터는 &lt;script type=&quot;math/tex&quot;&gt;x_1, \cdots, x_n&lt;/script&gt;개의 data point가 있고, 각각의 &lt;script type=&quot;math/tex&quot;&gt;x_i&lt;/script&gt;에 해당하는 실제값 &lt;script type=&quot;math/tex&quot;&gt;y_i&lt;/script&gt;가 존재한다. 그리고 우리는 다음과 같이 랜덤 노이즈가 섞인 함수 관계로 표현할 수 있다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;y = f(x) + \varepsilon, \text{where the noise} ~ \varepsilon \sim \mathcal{N}(0, \sigma^2)&lt;/script&gt;

&lt;p&gt;우리의 목표는 이 true function &lt;script type=&quot;math/tex&quot;&gt;f&lt;/script&gt;에 가능한 가장 근접한 prediction function &lt;script type=&quot;math/tex&quot;&gt;\hat{f}&lt;/script&gt;를 찾는 것이며, 이는 &lt;script type=&quot;math/tex&quot;&gt;y&lt;/script&gt;와 &lt;script type=&quot;math/tex&quot;&gt;\hat{f}&lt;/script&gt;의 &lt;a href=&quot;https://en.wikipedia.org/wiki/Mean_squared_error&quot;&gt;mean squared error&lt;/a&gt;를 최소화하는 문제를 품으로써 구할 수 있다. 물론 주어진 &lt;script type=&quot;math/tex&quot;&gt;x_1, \cdots, x_n&lt;/script&gt;뿐만 아니라 주어진 샘플 밖의 data points 에 대해서도 말이다. 그러나 &lt;script type=&quot;math/tex&quot;&gt;y_i&lt;/script&gt;가 갖고있는 랜덤 노이즈(&lt;script type=&quot;math/tex&quot;&gt;\varepsilon&lt;/script&gt;; &lt;em&gt;irreducible error&lt;/em&gt;) 때문에 완벽한 &lt;script type=&quot;math/tex&quot;&gt;\hat{f}&lt;/script&gt;를 찾는건 불가능함을 알아야 한다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\min \sum{ \big[ (y - \hat{f})^2 \big] }&lt;/script&gt;

&lt;p&gt;일반화된 &lt;script type=&quot;math/tex&quot;&gt;\hat{f}&lt;/script&gt;를 찾는 것은 다수의 샘플링된 데이터셋에 대한 다수의 알고리즘에 의해 진행되기 때문에, 우리가 어떤 &lt;script type=&quot;math/tex&quot;&gt;\hat{f}&lt;/script&gt;를 선택하든지 간에 unseen sample &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt;에 대한 기대오차(expected error)를 구할 수 있고, 이는 아래와 같이 분해가 된다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
{\displaystyle {\begin{aligned}\operatorname {E} {\Big [}{\big (}y-{\hat {f}}(x){\big )}^{2}{\Big ]}&amp;=\operatorname {Bias} {\big [}{\hat {f}}(x){\big ]}^{2}+\operatorname {Var} {\big [}{\hat {f}}(x){\big ]}+\sigma ^{2}\\\end{aligned}}} %]]&gt;&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;(\text{mse} = \text{Bias}^2 + \text{Variance} + \text{irreducible error})&lt;/script&gt;

&lt;h4 id=&quot;derivation&quot;&gt;Derivation&lt;/h4&gt;
&lt;p&gt;먼저 유도과정에 사용되는 수식을 나열하면 다음과 같다.&lt;/p&gt;

&lt;p&gt;i. 분산식의 재정렬&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;{\displaystyle {\begin{aligned}\operatorname {Var} [X]=\operatorname {E} [X^{2}]-\operatorname {E} [X]^{2} \Longleftrightarrow \operatorname {E} [X^{2}]=\operatorname {Var} [X]+\operatorname {E} [X]^{2}\end{aligned}}}&lt;/script&gt;

&lt;p&gt;ii. &lt;script type=&quot;math/tex&quot;&gt;y&lt;/script&gt;의 기댓값&lt;br /&gt;
ture function &lt;script type=&quot;math/tex&quot;&gt;f&lt;/script&gt;는 deterministic(결정된 함수)이므로 &lt;script type=&quot;math/tex&quot;&gt;\operatorname{E}[f] = f&lt;/script&gt;이다.&lt;br /&gt;
irreducible error &lt;script type=&quot;math/tex&quot;&gt;\varepsilon \sim \mathcal{N}(0, \sigma^2)&lt;/script&gt; 이므로 &lt;script type=&quot;math/tex&quot;&gt;\operatorname{E} [\varepsilon] = 0&lt;/script&gt;이다. 따라서,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;{\displaystyle \operatorname {E} [y]=\operatorname {E} [f+\varepsilon ]=\operatorname {E} [f]=f}.&lt;/script&gt;

&lt;p&gt;iii. &lt;script type=&quot;math/tex&quot;&gt;y&lt;/script&gt;의 분산&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;\operatorname{Var} [\varepsilon] = \sigma^2&lt;/script&gt;이며, (i) &amp;amp; (ii) 에 의해,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
{\displaystyle {\begin{aligned}\operatorname {Var} [y]&amp;=\operatorname {E} [(y-\operatorname {E} [y])^{2}]=\operatorname {E} [(y-f)^{2}]\\&amp;=\operatorname {E} [(f+\varepsilon -f)^{2}]=\operatorname {E} [\varepsilon ^{2}]\\&amp;=\operatorname {Var} [\varepsilon ]+\operatorname {E} [\varepsilon ]^{2}\\&amp;=\sigma ^{2}\end{aligned}}} %]]&gt;&lt;/script&gt;

&lt;p&gt;따라서 &lt;strong&gt;Bias-Variance Decomposition&lt;/strong&gt;은 다음과 같이 쓸 수 있다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
{\displaystyle {\begin{aligned}\operatorname {E} {\big [}(y-{\hat {f}})^{2}{\big ]}&amp;=\operatorname {E} [y^{2}+{\hat {f}}^{2}-2y{\hat {f}}]\\&amp;=\operatorname {E} [y^{2}]+\operatorname {E} [{\hat {f}}^{2}]-\operatorname {E} [2y{\hat {f}}]\\&amp;=\operatorname {Var} [y]+\operatorname {E} [y]^{2}+\operatorname {Var} [{\hat {f}}]+\operatorname {E} [{\hat {f}}]^{2}-2f\operatorname {E} [{\hat {f}}]\\&amp;=\operatorname {Var} [y]+\operatorname {Var} [{\hat {f}}]+(f^{2}-2f\operatorname {E} [{\hat {f}}]+\operatorname {E} [{\hat {f}}]^{2})\\&amp;=\operatorname {Var} [y]+\operatorname {Var} [{\hat {f}}]+(f-\operatorname {E} [{\hat {f}}])^{2}\\&amp;=\sigma ^{2}+\operatorname {Var} [{\hat {f}}]+\operatorname {Bias} [{\hat {f}}]^{2}\\&amp;=\text{irreducible error} + \text{Variance} + \text{Bias}^2\end{aligned}}} %]]&gt;&lt;/script&gt;

&lt;h3 id=&quot;simulation-of-bias-variance-tradeoff&quot;&gt;Simulation of Bias-Variance Tradeoff&lt;/h3&gt;
&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Polynomial_regression&quot;&gt;Polynomial Regression&lt;/a&gt;에서 모형의 복잡도를 결정하는 것은 degrees of polynomial 이다. Over/Under fitting 을 방지하고 모형의 제대로된 학습을 위해선 degrees of polynomial 을 잘 조정해야하며, 이 값이 변함에 따라 편향과 분산은 어떻게 변하는지 확인할 수 있다.&lt;br /&gt;
그림에서 회색 점은 실제값(&lt;script type=&quot;math/tex&quot;&gt;y_i&lt;/script&gt;)를 나타내며, 빨간색 선이 우리가 찾고자 하는 true function(&lt;script type=&quot;math/tex&quot;&gt;f&lt;/script&gt;)이다. 연한 파란색 선은 개별 prediction function(&lt;script type=&quot;math/tex&quot;&gt;f_1, \cdots, f_n&lt;/script&gt;)이며, 진한 파란색 선은 N개의 prediction functions의 평균인 expected prediction function(&lt;script type=&quot;math/tex&quot;&gt;\operatorname{E}[\hat(f)]&lt;/script&gt;)을 나타낸다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;degree = 1&lt;/strong&gt;: degree 가 1인 경우 편향은 0.4010으로 다소 높은 값을 가지며, 분산은 0.3946으로 낮은 값을 갖는다. 이는 &lt;script type=&quot;math/tex&quot;&gt;\hat{f}&lt;/script&gt;가 너무 단순하여 주어진 데이터에 대해 학습이 제대로 이뤄지지 않았음을 보여준다. (Underfitting)&lt;/p&gt;

&lt;center&gt;&lt;img src=&quot;/images/bias-variance-1.png&quot; width=&quot;600&quot; /&gt;&lt;/center&gt;
&lt;center&gt;&lt;img src=&quot;/images/bias-variance-2.png&quot; width=&quot;350&quot; /&gt;&lt;/center&gt;

&lt;p&gt;&lt;strong&gt;degree = 4&lt;/strong&gt;: degree 가 4인 경우 편향은 0.1384로 degree 1 일 때보다 대폭 감소하였으며, 분산은 0.5529로 다소 증가하였다. 편향과 분산의 에러의 총 합을 보면 이전보다 0.1가량 줄어들어 &lt;script type=&quot;math/tex&quot;&gt;\hat{f}&lt;/script&gt;가 주어진 데이터에 대해 학습이 잘 이뤄졌음을 볼 수 있다. (Well-fitted)&lt;/p&gt;

&lt;center&gt;&lt;img src=&quot;/images/bias-variance-3.png&quot; width=&quot;600&quot; /&gt;&lt;/center&gt;
&lt;center&gt;&lt;img src=&quot;/images/bias-variance-4.png&quot; width=&quot;350&quot; /&gt;&lt;/center&gt;

&lt;p&gt;&lt;strong&gt;degree = 10&lt;/strong&gt;: degree 가 10인 경우 편향은 0.3811으로 degree 4일때보다 높게 나왔지만 이는 샘플의 수가 충분치 않아 생기는 현상으로 일반적으로 편향은 감소하게 된다. 반면에 분산은 9.7537로 대폭 상승하였으며, 이는 &lt;script type=&quot;math/tex&quot;&gt;\hat{f}&lt;/script&gt;가 너무 복잡하여 주어진 데이터가 가진 노이즈까지 학습하여 과적합이 이뤄졌음을 보여준다. (Overfitting)&lt;/p&gt;

&lt;center&gt;&lt;img src=&quot;/images/bias-variance-5.png&quot; width=&quot;600&quot; /&gt;&lt;/center&gt;
&lt;center&gt;&lt;img src=&quot;/images/bias-variance-6.png&quot; width=&quot;350&quot; /&gt;&lt;/center&gt;

&lt;p&gt;&lt;strong&gt;Train &amp;amp; Test Error&lt;/strong&gt;: 일반적으로 모형의 복잡도가 증가할수록 Train Set에 대해서 에러는 감소한다. 그러나 우리에게 중요한 것은 주어진 데이터셋에 대해 예측력을 높이는 것이 아닌, 앞으로 모형을 활용할 Unseen Data에 대한 예측력이며 이는 Test Set에 대한 에러를 의미한다.&lt;br /&gt;
아래 그림에서와 같이 모형의 복잡도(Degree)가 증가할 수록 Train set에 대한 총 오차는 지속적으로 감소함을 볼 수 있고, Test set에 대한 총 오차는 degree = 4일때 최소가 되어 모형의 최적 degree는 4가 됨을 알 수 있다.&lt;/p&gt;

&lt;center&gt;&lt;img src=&quot;/images/bias-variance-8.png&quot; width=&quot;500&quot; /&gt;&lt;/center&gt;
&lt;center&gt;&lt;img src=&quot;/images/bias-variance-9.png&quot; width=&quot;500&quot; /&gt;&lt;/center&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;em&gt;reference&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://scott.fortmann-roe.com/docs/BiasVariance.html&quot;&gt;Understanding the Bias-Variance Tradeoff&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://norman3.github.io/prml/docs/chapter03/2.html&quot;&gt;Bias-Variance Decomposition&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff#cite_note-6&quot;&gt;Bias–variance tradeoff&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;em&gt;code&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/JKeun/something-fun/blob/master/notebooks/bias-variance-tradeoff.ipynb&quot;&gt;github.com/JKeun&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Thu, 03 May 2018 00:00:00 +0900</pubDate>
        <link>https://jkeun.github.io/2018-05-03/bias-variance-tradeoff/</link>
        <guid isPermaLink="true">https://jkeun.github.io/2018-05-03/bias-variance-tradeoff/</guid>
      </item>
    
      <item>
        <title>Degrees of Freedom</title>
        <description>&lt;center&gt;&lt;mark&gt;&quot;In statistics, the number of values in the final calculation of a statistic that are free to vary.&quot;&lt;/mark&gt;&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;통계학의 여러 문제들을 풀기 위해선 &lt;a href=&quot;https://en.wikipedia.org/wiki/Degrees_of_freedom_(statistics)&quot;&gt;자유도 ( Degrees of Freedom )&lt;/a&gt;를 결정해야 할 때가 있다. 자유도는 “계산에 사용되는 자유로운 데이터의 수” 라고 정의 한다. 그렇다면 도대체 자유로운 데이터는 무엇이며, 자유롭지 않은 데이터는 무엇이란 말인가? 자유도에 대한 개념을 설명하기 위해 항상 나오는 &lt;a href=&quot;https://en.wikipedia.org/wiki/Mean&quot;&gt;평균 ( Mean )&lt;/a&gt;을 구하는 예를 살펴보자.&lt;/p&gt;

&lt;h5 id=&quot;an-illustration-with-a-sample-mean&quot;&gt;AN ILLUSTRATION WITH A SAMPLE MEAN&lt;/h5&gt;
&lt;p&gt;우리 모두가 알듯이 평균은 모든 데이터를 더한 후, 데이터의 총 갯수만큼 나눠서 구할 수 있다. 그렇다면 예를 들어 우리가 갖고 있는 데이터의 평균이 &lt;script type=&quot;math/tex&quot;&gt;25&lt;/script&gt; 이고, 데이터의 값들이 &lt;script type=&quot;math/tex&quot;&gt;20, 10, 50,&lt;/script&gt; 그리고 하나의 &lt;em&gt;unknown value&lt;/em&gt; 가 있다고 하자. 여기서 &lt;em&gt;unknown value&lt;/em&gt; 를 구하는 방정식은 &lt;script type=&quot;math/tex&quot;&gt;(20 + 10 + 50 + x) / 4 = 25&lt;/script&gt; 으로 세울 수 있고, 이를 풀면 &lt;script type=&quot;math/tex&quot;&gt;x = 20&lt;/script&gt; 으로 결정&lt;em&gt;( determined )&lt;/em&gt; 된다.&lt;/p&gt;

&lt;p&gt;그럼 더 나아가서 평균이 &lt;script type=&quot;math/tex&quot;&gt;25&lt;/script&gt; 이고, 데이터의 값들이 &lt;script type=&quot;math/tex&quot;&gt;20, 10,&lt;/script&gt; 그리고 두 개의 &lt;em&gt;unknown values&lt;/em&gt; 가 있다고 하자. 이 두 &lt;em&gt;unknown values&lt;/em&gt; 는 다를 수도 있기 때문에, 두 &lt;em&gt;different variables&lt;/em&gt; 은 &lt;script type=&quot;math/tex&quot;&gt;x, y&lt;/script&gt; 로 정의한다. 그리고 방정식을 세우면 &lt;script type=&quot;math/tex&quot;&gt;(20 + 10 + x + y) = 25&lt;/script&gt; 이 되고, 이를 풀면 &lt;script type=&quot;math/tex&quot;&gt;y = 70 - x&lt;/script&gt; 가 된다. 이 식은 일단 우리가 &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt; 의 값을 선택하면, &lt;script type=&quot;math/tex&quot;&gt;y&lt;/script&gt; 의 값은 완전히 결정됨을 의미한다. 즉, 현재 우리는 &lt;strong&gt;&lt;em&gt;한 번의 선택 ( one choice )&lt;/em&gt;&lt;/strong&gt; 을 할 수 있고, 그것은 &lt;strong&gt;&lt;em&gt;하나의 자유도 ( one degree of freedom )&lt;/em&gt;&lt;/strong&gt; 가 생겼음을 의미한다.&lt;/p&gt;

&lt;p&gt;그럼 더더더 나아가서 &lt;script type=&quot;math/tex&quot;&gt;100&lt;/script&gt;개의 샘플을 갖고 있다고 가정하자. 만약 우리가 평균이 &lt;script type=&quot;math/tex&quot;&gt;20&lt;/script&gt; 이라는 것만 알고 나머지 데이터 값들은 모른다면, 그 때의 자유도는 &lt;script type=&quot;math/tex&quot;&gt;99&lt;/script&gt; 가 되고, 모든 값들을 다 더한다면 &lt;script type=&quot;math/tex&quot;&gt;20 \times 100 = 2000&lt;/script&gt; 이 될 것이다. 또한 우리가 &lt;script type=&quot;math/tex&quot;&gt;99&lt;/script&gt; 개의 데이터 값들을 안다면, 마지막 한 개의 값은 자동으로 결정될 것이다.&lt;/p&gt;

&lt;h5 id=&quot;standard-deviation&quot;&gt;STANDARD DEVIATION&lt;/h5&gt;
&lt;p&gt;통계학을 배우면서 자유도의 개념을 가장 먼저 접하는 순간이 &lt;a href=&quot;https://en.wikipedia.org/wiki/Standard_deviation&quot;&gt;표준편차 ( Stnadard deviation )&lt;/a&gt;를 배울 때다. 그럼 먼저 ( 샘플 ) 표준편차의 식을 보면서 살펴보기로 하자.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;s = \sqrt{\frac{1}{N-1} \sum (x_i - \bar{x})^2}, \quad df: n - 1&lt;/script&gt;

&lt;p&gt;흔히 ( 샘플 ) 표준편차를 구할 때, 평균과 ( mean : &lt;script type=&quot;math/tex&quot;&gt;\bar{x}&lt;/script&gt; ) 각 관찰값들의 차이의 합을 &lt;script type=&quot;math/tex&quot;&gt;N-1&lt;/script&gt; 으로 나누어야 우리가 원하는 ( unbiased ) 표준편차가 나온다. ( 불편추정량에 대해서는 &lt;a href=&quot;https://en.wikipedia.org/wiki/Bias_of_an_estimator&quot;&gt;Bias of an estimator&lt;/a&gt; 를 참고하자. )&lt;/p&gt;

&lt;p&gt;여기서 자유도가 &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt; 이 아니라 &lt;script type=&quot;math/tex&quot;&gt;n-1&lt;/script&gt; 인 이유는, &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt; 개의 관찰값 ( &lt;em&gt;unknown values&lt;/em&gt; ) 과 평균 ( sample mean : &lt;script type=&quot;math/tex&quot;&gt;\bar{x}&lt;/script&gt;) 이 위 공식에 쓰여졌기 때문이다. 즉, 표준편차를 구하기 위해 사용된 평균을 우리는 이미 알고, 이로 인해 &lt;script type=&quot;math/tex&quot;&gt;1&lt;/script&gt; 개의 관찰값은 자동으로 결정 ( &lt;em&gt;determined&lt;/em&gt; ) 되기 때문에 &lt;script type=&quot;math/tex&quot;&gt;n-1&lt;/script&gt; 개의 자유도를 갖게 된 것이다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;“다시말해 어떤 &lt;em&gt;수식값 ( 통계량 )&lt;/em&gt; 을 찾기 위해 &lt;em&gt;사용된 parameter 의 갯수 ( &lt;script type=&quot;math/tex&quot;&gt;k&lt;/script&gt; )&lt;/em&gt; 를 &lt;em&gt;샘플의 크기 ( &lt;script type=&quot;math/tex&quot;&gt;N&lt;/script&gt; )&lt;/em&gt; 에서 빼준 것이 &lt;em&gt;자유도의 수 ( &lt;script type=&quot;math/tex&quot;&gt;N-k&lt;/script&gt; )&lt;/em&gt; 가 된다.”&lt;/strong&gt;&lt;/p&gt;

&lt;h5 id=&quot;sum-of-squares&quot;&gt;SUM OF SQUARES&lt;/h5&gt;
&lt;p&gt;회귀분석에서 &lt;script type=&quot;math/tex&quot;&gt;R^2&lt;/script&gt;(결정계수) 를 구하거나 회귀모형의 &lt;script type=&quot;math/tex&quot;&gt;F&lt;/script&gt;-검정을 할 때, 나오는 개념이 &lt;a href=&quot;https://en.wikipedia.org/wiki/Partition_of_sums_of_squares&quot;&gt;제곱합(SUM OF SQUARES)&lt;/a&gt; 이다.&lt;/p&gt;
&lt;center&gt;&lt;img src=&quot;https://image.slidesharecdn.com/linearregression-140903114216-phpapp01/95/linear-regression-22-638.jpg?cb=1409744639&quot; width=&quot;400&quot; /&gt;&lt;/center&gt;

&lt;ul&gt;
  &lt;li&gt;결정계수 ( &lt;script type=&quot;math/tex&quot;&gt;R^2&lt;/script&gt;; Coefficient of Determination ) : &lt;script type=&quot;math/tex&quot;&gt;\frac{SSR}{SST} = 1 - \frac{SSE}{SST}&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;총제곱합 ( SST; Total Sum of Squres ) : &lt;script type=&quot;math/tex&quot;&gt;\sum{(Y_i - \bar{Y})^2}&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;오차제곱합 ( SSE; Sum of Squres Error ) : &lt;script type=&quot;math/tex&quot;&gt;\sum{(Y_i - \hat{Y_i})^2}&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;회귀제곱합 ( SSR; Sum of Squres Regression ) : &lt;script type=&quot;math/tex&quot;&gt;\sum{(\hat{Y_i} - \bar{Y})^2}&lt;/script&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;우리가 추정한 회귀식이 표본을 잘 설명하고 있다면 설명된 제곱합 SSR 은 설명 안 된 제곱합 SSE 에 비해 상대적으로 클 것이다. 따라서 결정계수 ( &lt;script type=&quot;math/tex&quot;&gt;R^2&lt;/script&gt; ) 는 &lt;script type=&quot;math/tex&quot;&gt;1&lt;/script&gt; 에 가까워지게 된다. 반대로 회귀식이 표본을 잘 설명하지 않는다면 설명 안 된 제곱합 SSE 이 설명된 제곱합 SSR 에 비해 상대적으로 크게 되어 결정계수는 &lt;script type=&quot;math/tex&quot;&gt;0&lt;/script&gt; 에 가까워지게 된다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
제곱합 \quad &amp;SST = SSE + SSR \\
자유도 \quad &amp;n-1 = (n-k-1) + (k) \\
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;그렇다면 각 제곱합들의 자유도는 어떻게 될까? 먼저 SST 의 자유도는 수식에서 평균 ( &lt;script type=&quot;math/tex&quot;&gt;\bar{Y}&lt;/script&gt; ) 이라는 parameter 가 사용되었으므로 자유도는 &lt;script type=&quot;math/tex&quot;&gt;n -1&lt;/script&gt; 이 된다. SSE 의 자유도는 예측값 ( &lt;script type=&quot;math/tex&quot;&gt;\hat{Y_i} = \beta_0 + \beta_1 X_1 + \dots + \beta_k X_k&lt;/script&gt; ) 을 추정하기 위해 회귀식의 parameter 인 &lt;script type=&quot;math/tex&quot;&gt;\beta_0, \beta_1, \dots, \beta_k&lt;/script&gt; 가 사용되었으므로 자유도는 &lt;script type=&quot;math/tex&quot;&gt;n-k-1&lt;/script&gt; 이 된다. 마지막으로 SSR 의 자유도는 SST - SSE 이므로 자유도는 &lt;script type=&quot;math/tex&quot;&gt;k&lt;/script&gt; 가 된다. 이는 곧 회귀식에 사용한 독립변수 ( &lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt;, independent variables ) 의 갯수와 같다는 것을 알 수 있다.&lt;/p&gt;
</description>
        <pubDate>Mon, 24 Jul 2017 00:00:00 +0900</pubDate>
        <link>https://jkeun.github.io/2017-07-24/degrees-of-freedom/</link>
        <guid isPermaLink="true">https://jkeun.github.io/2017-07-24/degrees-of-freedom/</guid>
      </item>
    
      <item>
        <title>Project of Toyota Corolla Dataset ( 1 )</title>
        <description>&lt;center&gt;&lt;mark&gt;&quot;Simple is the Best&quot;&lt;/mark&gt;&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;“경제성의 원리”라고도 불리우는 &lt;a href=&quot;https://ko.wikipedia.org/wiki/%EC%98%A4%EC%BB%B4%EC%9D%98_%EB%A9%B4%EB%8F%84%EB%82%A0&quot;&gt;오컴의면도날&lt;/a&gt;과 같이 통계학적 모델링도 마찬가지이다. 어떤 현상을 설명할 때 불필요한 가정을 해서는 안 된다는 것. 고로 똑같은 성능을 내는 모델이라면, 단순하면 단순할수록 좋다.
이 원칙에 의거해 ( 분석자마다 원칙은 다를 수 있다. 적어도 나는 이러한 원칙을 항상 마음에 새기고 있을 뿐이다. ) 분석을 하기 전 고려해야할 사항들은 아래와 같을 것이다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;How many features do you have ? ( to build your model )&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;How much score do you want ? ( to explain your model )&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;How can you test your model ? ( for generalization )&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;
&lt;h4 id=&quot;workflow-of-regression&quot;&gt;WORKFLOW OF REGRESSION&lt;/h4&gt;

&lt;h5 id=&quot;define-dataset&quot;&gt;DEFINE DATASET&lt;/h5&gt;
&lt;ul&gt;
  &lt;li&gt;Sample set&lt;/li&gt;
  &lt;li&gt;Train set&lt;/li&gt;
  &lt;li&gt;Validation set&lt;/li&gt;
  &lt;li&gt;Test set&lt;/li&gt;
&lt;/ul&gt;

&lt;h5 id=&quot;define-features&quot;&gt;DEFINE FEATURES&lt;/h5&gt;
&lt;center&gt;&lt;img src=&quot;/images/2017-06-15-project-regression-toyota-corolla-1.png&quot; /&gt;&lt;/center&gt;

&lt;h5 id=&quot;explore-features&quot;&gt;EXPLORE FEATURES&lt;/h5&gt;
&lt;p&gt;회귀분석 모형에 사용할 변수를 찾아가는 과정. &lt;br /&gt; 
주로 시각화 + Descriptive statistics(기술통계)를 통해 갖고있는 데이터 분포 및 특징(평균, 최빈값, 중앙값, 분산, 최대값, 최소값 등)을 파악한다. 이를 통해 모형을 설명 할 변수들의 중요도를 대략적으로 파악 할 수 있다. 또한 수많은 변수들 중에서 회귀모형에 사용 할 초기 변수들을 선택하는데 어느정도 기준을 세울 수 있다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;각각의 변수 별 경험적 결과가 어떠한지 정의하기&lt;/li&gt;
  &lt;li&gt;분포 파악하기
    &lt;ul&gt;
      &lt;li&gt;real value : 종속변수 ( &lt;script type=&quot;math/tex&quot;&gt;y&lt;/script&gt; ) 와 설명변수 ( &lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt; ) 간의 Scatter plot ( 또는 Pair plot ) + 상관계수 파악을 통해 상관정도를 가늠&lt;/li&gt;
      &lt;li&gt;categorical value : 설명변수의 등급별 종속변수 값의 Box plot + Paired Sample t-test 를 통해 유의한 차이가 있는지 파악&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h5 id=&quot;feature-selection--step-wise-methods-&quot;&gt;FEATURE SELECTION ( STEP-WISE METHODS )&lt;/h5&gt;

&lt;h5 id=&quot;outliers-and-influential-observations&quot;&gt;OUTLIERS AND INFLUENTIAL OBSERVATIONS&lt;/h5&gt;
&lt;p&gt;이상치 ( Outlier ) 는 데이터 분포에서 극단에 있는 Data point 를 말한다. 이상치가 있고, 없고에 따라 분포의 특성 ( 평균 ) 이 급격하게 움직이는데, 이러한 Data point 는 내가 갖고있는 데이터의 대표값에 나쁜 영향 ( Influential 이 강함 ) 을 주는 것이다. 반면에 대표값에 별다른 영향을 주지 않는다면 ( Inlfuence 가 약함 ) 해당 Data point 를 그대로 사용해도 무방하다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Leverage_(statistics)&quot;&gt;Leverage&lt;/a&gt; : 설명변수 &lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt; 값에 대한 이상치 파악&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Studentized_residual&quot;&gt;( Studentized ) Residual&lt;/a&gt; : 종속변수 &lt;script type=&quot;math/tex&quot;&gt;y&lt;/script&gt; 값에 대한 이상치 파악&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/DFFITS&quot;&gt;DFFITS&lt;/a&gt; : 추정치 ( &lt;script type=&quot;math/tex&quot;&gt;\hat{y}&lt;/script&gt; ) 에 대한 영향력 평가&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Cook%27s_distance&quot;&gt;COOK’S DISTANCE&lt;/a&gt; : 회귀계수에 대한 종합적 영향력 평가&lt;/li&gt;
&lt;/ul&gt;

&lt;h5 id=&quot;diagnosis-of-models-and-assumptions&quot;&gt;DIAGNOSIS OF MODELS AND ASSUMPTIONS&lt;/h5&gt;
&lt;ol&gt;
  &lt;li&gt;선형성 검토 : 독립변수에 대한 잔차를 통해 선형여부를 판단 ( &lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt; 를 대수변환, 지수변환 등 )&lt;/li&gt;
  &lt;li&gt;등분산성 검토 : &lt;script type=&quot;math/tex&quot;&gt;\hat{y}&lt;/script&gt; 에 대한 잔차를 통해 등분산성 판단 ( &lt;script type=&quot;math/tex&quot;&gt;y&lt;/script&gt; 를 대수변환, 제곱근변환 등 )&lt;/li&gt;
  &lt;li&gt;독립성 검토 : 오차항들은 서로 독립&lt;/li&gt;
&lt;/ol&gt;

&lt;h5 id=&quot;check-an-improved-model&quot;&gt;CHECK AN IMPROVED MODEL&lt;/h5&gt;

&lt;h5 id=&quot;diagnosis-of-multicollinearity&quot;&gt;DIAGNOSIS OF MULTICOLLINEARITY&lt;/h5&gt;
&lt;p&gt;독립변수 ( &lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt; ) 들 간에 서로 상관성이 높을 경우 &lt;a href=&quot;https://en.wikipedia.org/wiki/Multicollinearity&quot;&gt;Multicollinearity ( 다중공선성 )&lt;/a&gt; 이 발생한다. 머신러닝 문제에서 Overfitting 과 비슷한 개념. 이 경우, 모델에 학습되지 않은 범위의 &lt;script type=&quot;math/tex&quot;&gt;X_{new}&lt;/script&gt; 데이터가 들어올 경우 굉장히 불안정한 예측값 ( &lt;script type=&quot;math/tex&quot;&gt;\hat{y}&lt;/script&gt; ) 을 내놓는다. 모델링의 목적인 Generalization 에 위배.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Pearson_correlation_coefficient&quot;&gt;( Pearson ) 상관행렬 및 상관계수&lt;/a&gt;를 통해 독립변수간 상관성 파악&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors&quot;&gt;고유값 ( Eigen value ) &lt;/a&gt; &amp;lt; &lt;script type=&quot;math/tex&quot;&gt;30&lt;/script&gt; &amp;amp; &lt;a href=&quot;https://en.wikipedia.org/wiki/Variance_inflation_factor&quot;&gt;분산팽창계수 ( VIF; Variance Inflation Factor )&lt;/a&gt; &amp;lt; &lt;script type=&quot;math/tex&quot;&gt;10&lt;/script&gt; 검토&lt;/li&gt;
&lt;/ul&gt;

&lt;h5 id=&quot;find-a-final-regression-model&quot;&gt;FIND A FINAL REGRESSION MODEL&lt;/h5&gt;
</description>
        <pubDate>Thu, 15 Jun 2017 00:00:00 +0900</pubDate>
        <link>https://jkeun.github.io/2017-06-15/project-regression-toyota-corolla-1/</link>
        <guid isPermaLink="true">https://jkeun.github.io/2017-06-15/project-regression-toyota-corolla-1/</guid>
      </item>
    
  </channel>
</rss>
